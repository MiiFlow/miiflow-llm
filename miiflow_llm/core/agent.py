"""Unified Agent architecture focused on LLM reasoning (stateless)."""

import asyncio
import time
import logging
from typing import Any, Dict, List, Optional, Type, TypeVar, Generic, Callable, Union, AsyncIterator
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum

logger = logging.getLogger(__name__)

# Import observability components
try:
    from .observability.context import TraceContext, get_current_trace_context
    OBSERVABILITY_AVAILABLE = True
except ImportError:
    OBSERVABILITY_AVAILABLE = False
    TraceContext = None

from .client import LLMClient
from .message import Message, MessageRole
from .tools import FunctionTool, ToolRegistry
from .exceptions import MiiflowLLMError, ErrorType


Deps = TypeVar('Deps')
Result = TypeVar('Result')


class AgentType(Enum):
    SINGLE_HOP = "single_hop"      # Simple, direct response
    REACT = "react"                # ReAct with multi-hop reasoning


@dataclass
class RunResult(Generic[Result]):
    data: Result
    messages: List[Message]
    all_messages: List[Message] = field(default_factory=list)
    
    def __post_init__(self):
        if not self.all_messages:
            self.all_messages = self.messages


@dataclass
class RunContext(Generic[Deps]):
    """Context passed to tools and agent functions (stateless)."""

    deps: Deps
    messages: List[Message] = field(default_factory=list)
    retry: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)
    trace_context: Optional[TraceContext] = None

    def __post_init__(self):
        """Initialize trace context if available."""
        if OBSERVABILITY_AVAILABLE and self.trace_context is None:
            # Get current trace context or create a new one
            current_context = get_current_trace_context()
            if current_context:
                self.trace_context = current_context.child_context()

    def with_trace_context(self, trace_context: Optional[TraceContext]) -> "RunContext[Deps]":
        """Create a copy with a specific trace context."""
        return RunContext(
            deps=self.deps,
            messages=self.messages.copy(),
            retry=self.retry,
            metadata=self.metadata.copy(),
            trace_context=trace_context
        )
    
    def last_user_message(self) -> Optional[Message]:
        """Get the last user message."""
        for msg in reversed(self.messages):
            if msg.role == MessageRole.USER:
                return msg
        return None
    
    def last_agent_message(self) -> Optional[Message]:
        """Get the last agent message."""
        for msg in reversed(self.messages):
            if msg.role == MessageRole.ASSISTANT:
                return msg
        return None
    
    def get_conversation_summary(self) -> str:
        """Get a summary of the current conversation context."""
        if len(self.messages) <= 2:
            return "New conversation"
        
        user_messages = [msg.content for msg in self.messages if msg.role == MessageRole.USER]
        return f"Conversation with {len(user_messages)} user messages"


class Agent(Generic[Deps, Result]):
    """Unified Agent focused on LLM reasoning (stateless)."""
    
    def __init__(
        self,
        client: LLMClient,
        *,
        agent_type: AgentType = AgentType.SINGLE_HOP,
        deps_type: Optional[Type[Deps]] = None,
        result_type: Optional[Type[Result]] = None,
        system_prompt: Optional[Union[str, Callable[[RunContext[Deps]], str]]] = None,
        retries: int = 1,
        max_iterations: int = 10,
        temperature: float = 0.7,
        tools: Optional[List[FunctionTool]] = None
    ):
        self.client = client
        self.agent_type = agent_type
        self.deps_type = deps_type
        self.result_type = result_type or str
        self.system_prompt = system_prompt
        self.retries = retries
        self.max_iterations = max_iterations
        self.temperature = temperature
        
        # Share the tool registry with LLMClient for consistency
        self.tool_registry = self.client.tool_registry
        self._tools: List[FunctionTool] = []
        
        # Register provided tools
        if tools:
            for tool in tools:
                self.tool_registry.register(tool)
                self._tools.append(tool)
        
    
    def add_tool(self, func: Callable) -> None:
        """Add a tool function (decorated with global @tool) to this agent.
        
        Usage:
        from miiflow_llm.core.tools import tool
        
        @tool("search", "Search the web")
        def search_web(query: str) -> str:
            return search_results
            
        agent.add_tool(search_web)
        """
        from .tools.decorators import get_tool_from_function
        
        tool_instance = get_tool_from_function(func)
        if not tool_instance:
            raise ValueError(f"Function {func.__name__} is not decorated with @tool")
        
        self.tool_registry.register(tool_instance)
        self._tools.append(tool_instance)
        
        logger.debug(f"Added tool '{tool_instance.name}' to agent")
    
    async def run(
        self,
        user_prompt: str,
        *,
        deps: Optional[Deps] = None,
        message_history: Optional[List[Message]] = None
    ) -> RunResult[Result]:
        """Run the agent with dependency injection (stateless)."""

        # Get tracer for observability using standard OpenTelemetry
        tracer = None
        if OBSERVABILITY_AVAILABLE:
            from .observability.context import set_trace_context
            try:
                from opentelemetry import trace
                tracer = trace.get_tracer(__name__)
            except ImportError:
                tracer = None

        context = RunContext(
            deps=deps,
            messages=message_history or []
        )

        # Set trace context if available
        if tracer and context.trace_context:
            set_trace_context(context.trace_context)
        
        # Add system prompt if provided
        if self.system_prompt:
            if callable(self.system_prompt):
                system_content = self.system_prompt(context)
            else:
                system_content = self.system_prompt
            
            system_msg = Message(role=MessageRole.SYSTEM, content=system_content)
            context.messages.append(system_msg)
        
        # Add user message
        user_msg = Message(role=MessageRole.USER, content=user_prompt)
        context.messages.append(user_msg)
        # Execute with observability
        if tracer:
            with tracer.start_as_current_span(
                "agent.run",
                attributes={
                    "agent_type": self.agent_type.value,
                    "user_prompt": user_prompt[:200] + "..." if len(user_prompt) > 200 else user_prompt,
                    "message_history_count": len(message_history) if message_history else 0,
                    "max_retries": self.retries,
                    "tools_count": len(self._tools)
                }
            ) as span:
                return await self._execute_with_retries(context, tracer, span)
        else:
            return await self._execute_with_retries(context, None, None)

    async def _execute_with_retries(self, context: RunContext[Deps], tracer, span) -> RunResult[Result]:
        """Execute agent with retries and observability."""
        # Execute with retries
        for attempt in range(self.retries):
            context.retry = attempt
            try:
                if span:
                    span.add_event(f"agent_attempt_{attempt + 1}", {
                        "attempt": attempt + 1,
                        "max_retries": self.retries
                    })

                result = await self._execute_with_context(context)

                if span:
                    span.add_event("agent_execution_success", {
                        "attempt": attempt + 1,
                        "final_attempt": True
                    })

                return RunResult(
                    data=result,
                    messages=context.messages,
                    all_messages=context.messages.copy()
                )

            except Exception as e:
                if span:
                    span.add_event("agent_execution_error", {
                        "attempt": attempt + 1,
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "is_final_attempt": attempt == self.retries - 1
                    })

                if attempt == self.retries - 1:
                    if span:
                        span.attributes["error"] = True
                        span.attributes["error_message"] = str(e)
                    raise MiiflowLLMError(f"Agent failed after {self.retries} retries: {e}", ErrorType.MODEL_ERROR)
                continue
        raise MiiflowLLMError("Agent execution failed", ErrorType.MODEL_ERROR)
    
    async def _execute_with_context(self, context: RunContext[Deps]) -> Result:
        """Route to appropriate execution based on agent type."""
        if self.agent_type == AgentType.SINGLE_HOP:
            # Extract user prompt from context messages
            user_prompt = ""
            for msg in reversed(context.messages):
                if msg.role == MessageRole.USER:
                    user_prompt = msg.content
                    break

            final_answer = None
            all_events = []
            async for event in self.stream_single_hop(user_prompt, context=context):
                all_events.append(event)
                if isinstance(event, dict) and event.get("event") == "execution_complete":
                    final_answer = event.get("data", {}).get("result", "")
            
            logger.debug(f"Consumed {len(all_events)} events from stream_single_hop")

            if self.result_type == str:
                return final_answer or "No final answer received"
            else:
                return final_answer or "No final answer received"
        else:  # AgentType.REACT
            # Extract user prompt from context messages
            user_prompt = ""
            for msg in reversed(context.messages):
                if msg.role == MessageRole.USER:
                    user_prompt = msg.content
                    break

            final_answer = None
            async for event in self.stream_react(user_prompt, context, max_steps=self.max_iterations):
                if event.event_type.value == "final_answer":
                    final_answer = event.data.get("answer", "")
                    break

            if self.result_type == str:
                return final_answer or "No final answer received"
            else:
                return final_answer or "No final answer received"
    
    
    async def _execute_tool_calls(
        self, 
        tool_calls: List[Dict[str, Any]], 
        context: RunContext[Deps]
    ) -> None:
        """Execute tool calls with dependency injection."""
        logger.debug(f"About to execute {len(tool_calls)} tool calls")
        
        for i, tool_call in enumerate(tool_calls):
            logger.debug(f"Executing tool call {i+1}/{len(tool_calls)}")
            
            # Extract tool name and arguments
            if hasattr(tool_call, 'function'):
                tool_name = tool_call.function.name
                tool_args = tool_call.function.arguments
                if isinstance(tool_args, str) and tool_args.strip():
                    import json
                    tool_args = json.loads(tool_args)
                elif not tool_args or (isinstance(tool_args, str) and not tool_args.strip()):
                    tool_args = {}
            else:
                tool_name = tool_call.get("function", {}).get("name")
                tool_args = tool_call.get("function", {}).get("arguments", {})
                if isinstance(tool_args, str) and tool_args.strip():
                    import json
                    tool_args = json.loads(tool_args)
                elif not tool_args or (isinstance(tool_args, str) and not tool_args.strip()):
                    tool_args = {}
            
            if tool_args is None:
                tool_args = {}
            elif not isinstance(tool_args, dict):
                logger.warning(f"Invalid tool_args type: {type(tool_args)}, converting to empty dict")
                tool_args = {}
            
            logger.debug(f"Tool '{tool_name}' with args: {tool_args}")
            
            # Execute tool with context injection if needed
            tool = self.tool_registry.tools.get(tool_name)
            if tool and hasattr(tool, 'context_injection'):
                injection_pattern = tool.context_injection
                
                if injection_pattern['pattern'] == 'first_param':
                    logger.debug(f"Using context injection for {tool_name}")
                    observation = await self.tool_registry.execute_safe_with_context(
                        tool_name, context, **tool_args
                    )
                else:
                    logger.debug(f"Plain function execution for {tool_name}")
                    observation = await self.tool_registry.execute_safe(tool_name, **tool_args)
            else:
                logger.debug(f"Plain function execution (no pattern detection) for {tool_name}")
                observation = await self.tool_registry.execute_safe(tool_name, **tool_args)
            
            logger.debug(f"Tool '{tool_name}' execution result: success={observation.success}, output='{observation.output}'")
            
            # Add tool result message
            context.messages.append(Message(
                role=MessageRole.TOOL,
                content=str(observation.output) if observation.success else observation.error,
                tool_call_id=tool_call.id if hasattr(tool_call, 'id') else tool_call.get("id")
            ))
    
    async def stream_react(
        self,
        query: str,
        context: RunContext,
        max_steps: int = 10,
        max_budget: Optional[float] = None,
        max_time_seconds: Optional[float] = None
    ):
        """Run agent in ReAct mode with streaming events."""
        from .react import ReActFactory
        
        orchestrator = ReActFactory.create_orchestrator(
            agent=self,
            max_steps=max_steps,
            max_budget=max_budget,
            max_time_seconds=max_time_seconds
        )
        
        # Real-time streaming setup
        event_queue = asyncio.Queue()

        def real_time_stream(event):
            """Stream events immediately as they're published."""
            try:
                event_queue.put_nowait(event)
            except asyncio.QueueFull:
                import logging
                logging.getLogger(__name__).warning("Event queue full, dropping event")

        orchestrator.event_bus.subscribe(real_time_stream)
        execution_task = asyncio.create_task(orchestrator.execute(query, context))

        try:
            while not execution_task.done():
                try:
                    event = await asyncio.wait_for(event_queue.get(), timeout=0.1)
                    yield event  
                    event_queue.task_done()
                except asyncio.TimeoutError:
                    continue

            while not event_queue.empty():
                try:
                    event = event_queue.get_nowait()
                    yield event
                    event_queue.task_done()
                except asyncio.QueueEmpty:
                    break
            await execution_task

        finally:
            orchestrator.event_bus.unsubscribe(real_time_stream)
    
    async def stream_single_hop(
        self,
        user_prompt: str,
        *,
        context: RunContext[Deps]
    ) -> AsyncIterator[Dict[str, Any]]:
        """Stream single-hop execution - uses context from run() (no duplication)."""
        
        # Context is provided by run() - no setup duplication!
        
        yield {
            "event": "execution_start",
            "data": {
                "prompt": user_prompt,
                "context_length": len(context.messages),
                "tools_available": len(self._tools)
            }
        }
        
        try:
            yield {"event": "llm_start", "data": {}}
            
            buffer = ""
            final_tool_calls = None
            has_tool_calls = False

            # Stream LLM response - consume ALL chunks to ensure metrics are recorded
            async for chunk in self.client.astream_chat(
                messages=context.messages,
                tools=self._tools if self._tools else None,
                temperature=self.temperature
            ):
                if chunk.delta:
                    buffer += chunk.delta
                    yield {
                        "event": "llm_chunk",
                        "data": {"delta": chunk.delta, "content": buffer}
                    }

                # Check if we have tool calls
                if chunk.tool_calls:
                    has_tool_calls = True

                # Don't break - let the async generator finish to record metrics

            # If we had tool calls, get them properly by making a non-streaming call
            if has_tool_calls:
                response = await self.client.achat(
                    messages=context.messages,
                    tools=self._tools if self._tools else None,
                    temperature=self.temperature
                )
                final_tool_calls = response.message.tool_calls
            
            response_message = Message(
                role=MessageRole.ASSISTANT,
                content=buffer,
                tool_calls=final_tool_calls
            )
            context.messages.append(response_message)
            
            # If no tool calls and streaming didn't capture usage, make a final call
            # This ensures metrics are recorded (streaming often doesn't include usage)
            if not has_tool_calls:
                # Make a non-streaming call to record metrics properly
                await self.client.achat(
                    messages=context.messages[:-1],  # Exclude the message we just added
                    tools=self._tools if self._tools else None,
                    temperature=self.temperature
                )
            
            # Handle tool calls if present
            if final_tool_calls:
                yield {
                    "event": "tools_start",
                    "data": {"tool_count": len(final_tool_calls)}
                }
                
                await self._execute_tool_calls(final_tool_calls, context)
                
                yield {"event": "tools_complete", "data": {}}
                final_response = await self.client.achat(
                    messages=context.messages,
                    tools=None,
                    temperature=self.temperature
                )
                context.messages.append(final_response.message)
                result = final_response.message.content
            else:
                result = buffer
            
            yield {
                "event": "execution_complete",
                "data": {"result": result}
            }
            
        except Exception as e:
            yield {
                "event": "error",
                "data": {"error": str(e), "error_type": type(e).__name__}
            }
            raise
